import os
os.environ["DDE_BACKEND"] = "tensorflow.compat.v1"

import numpy as np
import tensorflow as tf   # See project README
import deepxde as dde     # See project README 


""" 
   Callback for saving the best soln. DeepXDE provides a callback for saving the best performing model to file, which can be restored
   at the end of the solve; however repeatedly writing to file is too slow. This callback was save the best performing
   model to memory. When an instance of this class is passed into model.train() as a callback, the solver will store the best performing
   weights in memory. The metric used is *test loss* (PDE component), not the train loss.  
"""
class InMemoryBestModel(dde.callbacks.Callback):
    def __init__(self):
        self.best_weights = None
        self.best_value = np.inf
        self.variables = None
        #self.debug_info_printed = False
        
    def set_model(self, model):
        self.model = model
        graph_vars = self.model.sess.graph.get_collection('trainable_variables')
        self.variables = graph_vars

        """
        if not self.debug_info_printed:
            print("=== DEBUG INFO ===")
            print(f"Model session type: {type(self.model.sess)}")
            print(f"Model graph type: {type(self.model.sess.graph)}")
            
            # Print available collections
            try:
                collections = self.model.sess.graph.get_all_collection_keys()
                print(f"Available collections: {collections}")
            except:
                print("Could not get collection keys")
            
            # Try to find variables
            try:
                graph_vars = self.model.sess.graph.get_collection('trainable_variables')
                print(f"Found {len(graph_vars)} trainable variables")
                self.variables = graph_vars
            except Exception as e:
                print(f"Could not get trainable_variables: {e}")
        """ 
               
    def on_epoch_end(self):
        try:
            value = self.model.train_state.loss_test[0]

            if value < self.best_value:
                self.best_value = value
                self.loss_train_at_best_loss_test = self.model.train_state.loss_train[0]
                self.best_weights = self.model.sess.run(self.variables)
                print(f"Saving weights for new best test loss: {value:.3e}")

                
        except Exception as e:
            print(f"Warning: Could not save best weights: {e}")

    def restore_best_weights(self):
        if self.best_weights is not None and self.variables is not None:
            try:
                assign_ops = [var.assign(weight) for var, weight in zip(self.variables, self.best_weights)]
                self.model.sess.run(assign_ops)
                print(f"Restoring weights corresponding to the best test loss {self.best_value:.3e}")
                print("(ignore output beginning 'Best model at step...' above, which was generated by DeepXDE)\n")
            except Exception as e:
                print(f"Warning: Could not restore weights: {e}")
                

            
            
def PINN_solve(rhs_func,                  
               boundary_condition_value, 
               num_output_points = 101,  
               num_colloc = 20,
               num_layers = 2,
               neurons_per_layer = 20, 
               num_epochs = 10000,
               train_distribution = "pseudo",
               use_best_test_loss = True,
               learning_rate_decay = True):
    """
    Solve u_xx = f(x) on [0,1], u(0)=0, u_x(1)=g, using a physics-informed neural network (PINN), using
    the DeepXDE library.

    Parameters
    ----------
    rhs_func : function
        The function f(x), should take in one input and provides one output
    boundary_condition_value : scalar
        The value g
    num_output_points : int, optional
        The number of points to provide the solution at. The default is 101.
    num_colloc : int, optional
        Number of collocation points (training points). The default is 20.
    num_layers : int, optional
        Number of layers in the neural network. The default is 2.
    neurons_per_layer : int, optional
        Number of neurons per layer (same in all layers). The default is 20.
    num_epochs : int, optional
        Number of epochs (iterations), default 10000
    train_distribution : string, optional
        How the collocation points will be distributed. Will be passed to dde.data.PDE(). Options
        include "uniform" or "pseudo" (default, randomly re-sampled each epoch)
    use_best_test_loss : boolean, optional
        Saves and restores the best performing weights, i.e., the weights for the iteration that provided 
        the lowest test loss. Defaults to True. 
    learning_rate_decay : boolean, optional
        If True, applies a learning rate decay schedule during training. The learning rate is halved after 
        a fixed number of epochs (every 1000), which can improve stability and reduce
        optimizer variance at later stages. Defaults to True.

    Returns
    -------
    x : numpy array
        Uniform grid of x values (size based on num_output_points)
    u : numpy array
        Solution at x values
    soln_info : dict
        Dictionary with the following entries
          - 'loss_train' Training points loss (PDE component) for the final solution
          - 'loss_test' Test points loss (PDE component) for the final solution
          - 'colloc_points' - array of collocation points
          - 'resid_at_colloc' - PDE residual at the collocation points
          - 'x_dense' - dense set of 10000 x values across the domain
          - 'resid_at_x_dense' - PDE residual at those x values (so can see if PDE residual is high between collocation points)
    """

    # define the PDE to be solved
    def pde(x, u):
        u_xx = dde.grad.hessian(u, x)
        return u_xx - rhs_func(x)

    # function defining the left boundary
    def boundary_left(x, on_boundary):
        return on_boundary and dde.utils.isclose(x[0], 0)
    
    # function defining the right boundary
    def boundary_right(x, on_boundary):
        return on_boundary and dde.utils.isclose(x[0], 1)

    # set up DeepXDE problem          
    geom = dde.geometry.Interval(0, 1)
    bc_l = dde.icbc.DirichletBC(geom, lambda X: 0.0, boundary_left)
    bc_r = dde.icbc.NeumannBC(geom, lambda x: boundary_condition_value, boundary_right)

    data = dde.data.PDE(geom, pde, [bc_l, bc_r], num_colloc, 2, num_test=100, train_distribution=train_distribution)

            
    # set up network
    layer_size = [1] + [neurons_per_layer] * num_layers + [1] # resolves to, e.g., [1, 20, 20, 1] if num_layers=2 and neurons_per_layer=20
    activation = "tanh"
    initializer = "Glorot uniform"
    net = dde.nn.FNN(layer_size, activation, initializer)
    
        
    model = dde.Model(data, net)
    
    # if learning_rate_decay is true, set up for learning rate halves every 1000 steps
    if learning_rate_decay:        
        # Exponential decay schedule
        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
            initial_learning_rate=0.001,
            decay_steps=1000,
            decay_rate=0.5,
            staircase=True,
        )
        
        # Adam optimizer with LR schedule
        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)        
        model.compile("adam", optimizer.learning_rate)
    else:
        model.compile("adam", lr=0.001)

    
    # if use_best_test_loss is true, set up so weights are saved each time the test loss is minimized
    if use_best_test_loss:
        best_model_cb = InMemoryBestModel()
        losshistory, train_state = model.train(iterations=num_epochs,callbacks=[best_model_cb])  # add e.g., display_every=500 to change how often the InMemoryBestModel CallBack runs
        # restore best perfomring model
        best_model_cb.restore_best_weights()
        final_loss_test = best_model_cb.best_value
        final_loss_train = best_model_cb.loss_train_at_best_loss_test
    else:
        losshistory, train_state = model.train(iterations=num_epochs)
        final_loss_train = train_state.loss_train[0]
        final_loss_test = train_state.loss_test[0]
    
    # compute soluntion on gridpoints
    x = np.linspace(0,1,num_output_points).reshape(-1,1)
    u = model.predict(x)
    
    x_dense = np.linspace(0, 1, 10000).reshape(-1,1)

    # collect other info to be returned
    soln_info = dict({'loss_train':final_loss_train,
                      'loss_test':final_loss_test,
                      'colloc_points':data.train_x_all,
                      'resid_at_colloc':model.predict(data.train_x_all, operator=pde),
                      'x_dense':x_dense,
                      'resid_at_x_dense':model.predict(x_dense, operator=pde)
                      })
    

    return x, u, soln_info